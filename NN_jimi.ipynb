{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425a9c7d-6587-4b50-b26c-7f9b1d0ccb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c37cf6-3604-400b-91c6-54d952c486ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8eb55a-d643-4ad3-a213-af27ecac7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908e525-51db-4c6a-bb3b-2a9eae8846a2",
   "metadata": {},
   "source": [
    "## Cuda Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad94346-0c2a-44ca-81d6-3326052c8ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avaliable:\t True\n",
      "current:\t 0\n",
      "device: \t <torch.cuda.device object at 0x000002203F5EECC8>\n",
      "count:\t\t 1\n",
      "name:\t\t NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "print('avaliable:\\t', torch.cuda.is_available())\n",
    "print('current:\\t', torch.cuda.current_device())\n",
    "print('device: \\t', torch.cuda.device(0))\n",
    "print('count:\\t\\t', torch.cuda.device_count())\n",
    "print('name:\\t\\t', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72845809-bfe0-458b-94cd-c3209ad7e180",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086b01e-28a6-4c8c-964e-8ba5b358bc13",
   "metadata": {},
   "source": [
    "###### Jimi's Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc339a5-a8ca-4f99-9044-8d584799e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/BT_Classification/backup/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a68d1-e345-47ec-afa0-3d010def0786",
   "metadata": {},
   "source": [
    "## Data Augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c7e993-9d60-402e-9421-1a2a0b870926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        path: path to file\n",
    "    Return:\n",
    "        img: image tensor\n",
    "    \"\"\"\n",
    "    img_nii = nib.load(path)\n",
    "    \n",
    "    img_np = np.squeeze(img_nii.get_fdata(dtype=np.float32))\n",
    "    \n",
    "    return torch.from_numpy(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01beb158-2c9d-4389-b2fc-d7c1a061b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crop_dim(full_size, crop_size):\n",
    "\n",
    "    if full_size[0] < crop_size[0] or full_size[1] < crop_size[1] or full_size[2] < crop_size[2]:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadf3995-fd51-4c30-b5bc-e6f201ec24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_crop(image_tensor, crop_size):\n",
    "    print(image_tensor.shape)\n",
    "    mask = np.where(image_tensor < torch.mean(image_tensor), 0, image_tensor)\n",
    "    mask[mask > 0] = 1\n",
    "    print(mask.shape)\n",
    "    c_s, c_w, c_h = crop_size\n",
    "    f_s, f_w, f_h = image_tensor.shape\n",
    "    \n",
    "    total = np.sum(mask)\n",
    "    \n",
    "    search = True\n",
    "    while search:\n",
    "        if f_s == c_s:\n",
    "            x = 0\n",
    "        else:\n",
    "            x = np.random.randint(f_s - c_s)\n",
    "\n",
    "        if f_w == c_w:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = np.random.randint(f_w - c_w)\n",
    "\n",
    "        if f_h == c_h:\n",
    "            z = 0\n",
    "        else:\n",
    "            z = np.random.randint(f_h - c_h)\n",
    "\n",
    "        cropped = mask[x:x + c_s, y:y + c_w, z:z + c_h]\n",
    "        \n",
    "        c_total = np.sum(cropped)\n",
    "        \n",
    "        if (c_total / total) > 0.1:\n",
    "            search = False\n",
    "    \n",
    "    return image_tensor[x:x + c_s, y:y + c_w, z:z + c_h]\n",
    "        \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72789cf4-fea6-4a60-9410-b1803a0b6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_crops(path, crop_size, count):\n",
    "    image_tensor = load_image(path)\n",
    "    \n",
    "    if check_crop_dim(image_tensor.shape, crop_size):\n",
    "        print('Crop size too large. Image:{} \\t crop:{}'.format(image_tensor.shape, crop_size))\n",
    "        return\n",
    "    \n",
    "    cropped = []\n",
    "    for i in range(count):\n",
    "        cropped.append(get_valid_crop(image_tensor, crop_size))\n",
    "        \n",
    "    return cropped\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a105e439-8455-4a5d-97fd-53081398040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([359, 99, 288])\n",
      "(359, 99, 288)\n",
      "torch.Size([359, 99, 288])\n",
      "(359, 99, 288)\n"
     ]
    }
   ],
   "source": [
    "crops = get_random_crops(path=path+'00000/FLAIR.nii', crop_size=(70, 70, 70), count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23bc6e-182b-4d83-9afc-5bf9965d7862",
   "metadata": {},
   "source": [
    "### View Croppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86109994-5e2f-4df7-ab20-dffeee3b3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_interactions.ipyplot as iplt\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b73c6c1-dd8e-46bd-afb3-cbde97699a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(image):\n",
    "    def axial(ax_slice):\n",
    "        return image[ax_slice,:,:]\n",
    "\n",
    "    def cor(cor_slice):\n",
    "        return image[:,cor_slice,:]\n",
    "\n",
    "    def sag(sag_slice):\n",
    "        return image[:,:,sag_slice]\n",
    "\n",
    "    # define layout\n",
    "    fig, ax = plt.subplots(3,1)\n",
    "\n",
    "    for a in ax:\n",
    "        a.xaxis.set_visible(False)\n",
    "        a.yaxis.set_visible(False)\n",
    "\n",
    "    ctrl1 = iplt.imshow(axial, ax_slice=np.arange(image.shape[2]), aspect='auto', ax=ax[0], vmin=0, vmax=1, cmap ='gray')\n",
    "    ctrl2 = iplt.imshow(sag, sag_slice=np.arange(image.shape[0]), aspect='auto', ax=ax[1], vmin=0, vmax=1, cmap ='gray')\n",
    "    crtl3 = iplt.imshow(cor, cor_slice=np.arange(image.shape[1]), aspect='auto', ax=ax[2], vmin=0, vmax=1, cmap ='gray' )\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cbfdaf7-ac74-49c6-afac-25d83da2fc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea77a6982e74cdbb9575dc59747f0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27f7b5bf1be44d6a5e3e2db0866dff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='ax_slice', max=69, readout=False), Label(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e4166051d449dfa7eb92cf0497d88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='sag_slice', max=69, readout=False), Label(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928c518d231f4966ac8374c375eed67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='cor_slice', max=69, readout=False), Label(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(crops[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98cb8079-47dc-46ed-bb41-9b8755c16b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c402f2f0304ceb85493e8ee40eb97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b249a7448758454480773036405b392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='ax_slice', max=69, readout=False), Label(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7441824afa42a68a237393265b4532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='sag_slice', max=69, readout=False), Label(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f40f04d1d4445ab99d541aeb64bf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='cor_slice', max=69, readout=False), Label(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(crops[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b2b67-505c-46d2-b185-75438afe3e95",
   "metadata": {},
   "source": [
    "## Building the 3D CNN and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b8cde15-1202-4a4e-8ed1-fbc90a63dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba33dca5-0efe-4047-9837-d4ef0613c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = load_image(path+'00000/FLAIR.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "200526f9-8b23-4922-913c-4c52f42fd0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([359, 99, 288])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e498029-8d2c-4207-8fa7-b14acdf973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         ...,\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278]],\n",
      "\n",
      "        [[0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         ...,\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278],\n",
      "         [0.0278, 0.0278, 0.0278,  ..., 0.0278, 0.0278, 0.0278]]])\n"
     ]
    }
   ],
   "source": [
    "print(image_tensor[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5190ee3-6544-4ef3-9655-02ece1176d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([359, 99, 288, 1])\n",
      "tensor([[[[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]]],\n",
      "\n",
      "\n",
      "        [[[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]],\n",
      "\n",
      "         [[0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          ...,\n",
      "          [0.0278],\n",
      "          [0.0278],\n",
      "          [0.0278]]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.unsqueeze(image_tensor, -1)\n",
    "print(tensor.shape)\n",
    "print(tensor[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0030faaa-f608-4ec8-952b-090ea177448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.Conv3d(1, 5, 5, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2ab112f-8170-41f5-a81a-2ab2856d76f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 355, 95, 284])\n"
     ]
    }
   ],
   "source": [
    "out = test(image_tensor.unsqueeze(0).unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47afbca5-1ae3-4e90-bd2d-eb0e429f843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_model():\n",
    "    in_channel = 1\n",
    "    base_out = 4\n",
    "    return nn.Sequential(nn.Conv3d(in_channels=in_channel, out_channels=base_out, kernel_size=3, bias=False),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out, base_out*2, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*2),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*2, base_out*4, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*4),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Conv3d(base_out*4, base_out*8, 3, bias=False),\n",
    "                         nn.MaxPool3d(2),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         \n",
    "                         \n",
    "                         nn.Conv3d(base_out*8, base_out*8, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*8),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*8, base_out*16, 3, bias=False),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.MaxPool3d(2),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         \n",
    "                         \n",
    "                         nn.Conv3d(base_out*16, base_out*16, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*16),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*16, base_out*32, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(256),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d594587b-e6ef-43f5-bf45-01cd1a84fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lin_model():\n",
    "    return nn.Sequential(nn.Linear(10616832, 64),\n",
    "#                          nn.Dropout2d(p=0.2),\n",
    "#                          nn.Linear(2048, 64),\n",
    "                         nn.Dropout2d(p=0.2),\n",
    "                         nn.Linear(64, 8),\n",
    "                         nn.Dropout2d(p=0.2),\n",
    "                         nn.Linear(8, 2)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b0e7aef-766a-48f0-9199-9436e9a88ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = make_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b4c7c1a3-799f-42c5-9eca-8a2bd1b1fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nn_model(image_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "367a47bc-48e3-4fe9-a0a6-c540e88b35dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 81, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa582bb9-8fda-4eeb-9754-2f2edead0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.flatten(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cbb8355d-2f92-4b16-aa0b-ca3f77338aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10616832])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e8988910-57c6-48c1-bbd0-b69f04bbde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = make_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6abd4985-58fb-474f-9a93-4882df33ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = lin_model(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7701c5d9-427d-47e9-a54d-0cb712c09028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb9f1cf4-c020-465f-99e7-15721eb61b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0144, -0.3824]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a40d8a-69fb-4179-9d70-ecd0136adbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
