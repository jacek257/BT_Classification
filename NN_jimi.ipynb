{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425a9c7d-6587-4b50-b26c-7f9b1d0ccb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c37cf6-3604-400b-91c6-54d952c486ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import nibabel as nib\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8eb55a-d643-4ad3-a213-af27ecac7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908e525-51db-4c6a-bb3b-2a9eae8846a2",
   "metadata": {},
   "source": [
    "## Cuda Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad94346-0c2a-44ca-81d6-3326052c8ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avaliable:\t True\n",
      "current:\t 0\n",
      "device: \t <torch.cuda.device object at 0x0000023D00276708>\n",
      "count:\t\t 1\n",
      "name:\t\t NVIDIA GeForce GTX 1060 6GB\n"
     ]
    }
   ],
   "source": [
    "print('avaliable:\\t', torch.cuda.is_available())\n",
    "print('current:\\t', torch.cuda.current_device())\n",
    "print('device: \\t', torch.cuda.device(0))\n",
    "print('count:\\t\\t', torch.cuda.device_count())\n",
    "print('name:\\t\\t', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72845809-bfe0-458b-94cd-c3209ad7e180",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f086b01e-28a6-4c8c-964e-8ba5b358bc13",
   "metadata": {},
   "source": [
    "###### Jimi's Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc339a5-a8ca-4f99-9044-8d584799e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'D:/BT_Classification/backup/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a68d1-e345-47ec-afa0-3d010def0786",
   "metadata": {},
   "source": [
    "## Data Augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c7e993-9d60-402e-9421-1a2a0b870926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        path: path to file\n",
    "    Return:\n",
    "        img: image tensor\n",
    "    \"\"\"\n",
    "    img_nii = nib.load(path)\n",
    "    \n",
    "    img_np = np.squeeze(img_nii.get_fdata(dtype=np.float32))\n",
    "    \n",
    "    return torch.from_numpy(img_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01beb158-2c9d-4389-b2fc-d7c1a061b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_crop_dim(full_size, crop_size):\n",
    "\n",
    "    if full_size[0] < crop_size[0] or full_size[1] < crop_size[1] or full_size[2] < crop_size[2]:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cadf3995-fd51-4c30-b5bc-e6f201ec24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_crop(image_tensor, crop_size):\n",
    "#     print(image_tensor.shape)\n",
    "    mask = np.where(image_tensor < torch.mean(image_tensor), 0, image_tensor)\n",
    "    mask[mask > 0] = 1\n",
    "#     print(mask.shape)\n",
    "    c_s, c_w, c_h = crop_size\n",
    "    f_s, f_w, f_h = image_tensor.shape\n",
    "    \n",
    "    total = np.sum(mask)\n",
    "    \n",
    "    search = True\n",
    "    while search:\n",
    "        if f_s == c_s:\n",
    "            x = 0\n",
    "        else:\n",
    "            x = np.random.randint(f_s - c_s)\n",
    "\n",
    "        if f_w == c_w:\n",
    "            y = 0\n",
    "        else:\n",
    "            y = np.random.randint(f_w - c_w)\n",
    "\n",
    "        if f_h == c_h:\n",
    "            z = 0\n",
    "        else:\n",
    "            z = np.random.randint(f_h - c_h)\n",
    "\n",
    "        cropped = mask[x:x + c_s, y:y + c_w, z:z + c_h]\n",
    "        \n",
    "        c_total = np.sum(cropped)\n",
    "        \n",
    "        if (c_total / total) > 0.1:\n",
    "            search = False\n",
    "    \n",
    "    return image_tensor[x:x + c_s, y:y + c_w, z:z + c_h]\n",
    "        \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72789cf4-fea6-4a60-9410-b1803a0b6afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_crops(path, crop_size, count, label_path):\n",
    "    if type(path) == str:\n",
    "        path = [path]\n",
    "    \n",
    "    labels = pd.read_csv(label_path, dtype={'BraTS21ID':str, 'MGMT_value':np.int64})\n",
    "#     print(labels.head())\n",
    "    cropped = []\n",
    "    \n",
    "    for i, p in enumerate(path):\n",
    "        print(i, p)\n",
    "        # SKIP BECAUSE SOMETHING WRONG\n",
    "        if '00053' in p:\n",
    "            print('\\tSkipped')\n",
    "            continue\n",
    "        if '00186' in p:\n",
    "            print('\\tSkipped')\n",
    "            continue\n",
    "            \n",
    "        # SKIP BECAUSE HOST SAID TO\n",
    "        if '00109' in p:\n",
    "            print('\\tSkipped')\n",
    "            continue\n",
    "        if '00123' in p:\n",
    "            print('\\tSkipped')\n",
    "            continue\n",
    "        if '00709' in p:\n",
    "            print('\\tSkipped')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        image_tensor = load_image(p)\n",
    "        p_id = re.split('\\\\\\\\|/', p)[-2]\n",
    "#         cropped.append([image_tensor, labels.loc[labels['BraTS21ID'] == p_id, 'MGMT_value'].iloc[0]])\n",
    "#         print(p_id)\n",
    "\n",
    "        if check_crop_dim(image_tensor.shape, crop_size):\n",
    "            print('\\tCrop size too large. Image:{} \\t crop:{}'.format(image_tensor.shape, crop_size))\n",
    "            continue\n",
    "\n",
    "        for i in range(count):\n",
    "            cropped.append([get_valid_crop(image_tensor, crop_size), labels.loc[labels['BraTS21ID'] == p_id, 'MGMT_value'].iloc[0]])\n",
    "    \n",
    "    data = torch.utils.data.DataLoader(cropped)\n",
    "    return cropped, data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a105e439-8455-4a5d-97fd-53081398040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "crops, t_data = get_random_crops(path=path+'00000\\FLAIR.nii', crop_size=(70, 70, 70), count=1, label_path=path+'/../../train_labels.csv')\n",
    "print(crops)\n",
    "print(t_data)\n",
    "\n",
    "for i, data in enumerate(t_data, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    \n",
    "    print(inputs.shape)\n",
    "    print(inputs.unsqueeze(-1).shape)\n",
    "    print(inputs.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23bc6e-182b-4d83-9afc-5bf9965d7862",
   "metadata": {},
   "source": [
    "### View Croppings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86109994-5e2f-4df7-ab20-dffeee3b3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_interactions.ipyplot as iplt\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b73c6c1-dd8e-46bd-afb3-cbde97699a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(image):\n",
    "    def axial(ax_slice):\n",
    "        return image[ax_slice,:,:]\n",
    "\n",
    "    def cor(cor_slice):\n",
    "        return image[:,cor_slice,:]\n",
    "\n",
    "    def sag(sag_slice):\n",
    "        return image[:,:,sag_slice]\n",
    "\n",
    "    # define layout\n",
    "    fig, ax = plt.subplots(3,1)\n",
    "\n",
    "    for a in ax:\n",
    "        a.xaxis.set_visible(False)\n",
    "        a.yaxis.set_visible(False)\n",
    "\n",
    "    ctrl1 = iplt.imshow(axial, ax_slice=np.arange(image.shape[2]), aspect='auto', ax=ax[0], vmin=0, vmax=1, cmap ='gray')\n",
    "    ctrl2 = iplt.imshow(sag, sag_slice=np.arange(image.shape[0]), aspect='auto', ax=ax[1], vmin=0, vmax=1, cmap ='gray')\n",
    "    crtl3 = iplt.imshow(cor, cor_slice=np.arange(image.shape[1]), aspect='auto', ax=ax[2], vmin=0, vmax=1, cmap ='gray' )\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27c71115-94bf-4ec1-b511-bfc3a15241a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c367feda2a4d47c88cd91643fd395902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32a061352c54c91b60c0eef0de6c025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='ax_slice', max=105, readout=False), Label(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3101eba9ee284b70b575c9b52dc71e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='sag_slice', max=358, readout=False), Label(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ce9706a7fc4429930b51a11180f3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='cor_slice', max=287, readout=False), Label(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(load_image(path+'00053/FLAIR.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c371a74e-b7ba-443f-bcdb-9daabd09b1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c52659acc9a4d8caca17e31f99da69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b548270f1c04337bb86fc49f916ddb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='ax_slice', max=78, readout=False), Label(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935df44a531f492b9a406655ecad9de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='sag_slice', max=262, readout=False), Label(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60dc8153ceab4021a6f917fdb4868ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntSlider(value=0, description='cor_slice', max=280, readout=False), Label(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view(load_image(path+'00186/FLAIR.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbfdaf7-ac74-49c6-afac-25d83da2fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "view(crops[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb8079-47dc-46ed-bb41-9b8755c16b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "view(crops[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b2b67-505c-46d2-b185-75438afe3e95",
   "metadata": {},
   "source": [
    "## Building the 3D CNN and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b8cde15-1202-4a4e-8ed1-fbc90a63dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33dca5-0efe-4047-9837-d4ef0613c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = load_image(path+'00000/FLAIR.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200526f9-8b23-4922-913c-4c52f42fd0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e498029-8d2c-4207-8fa7-b14acdf973e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_tensor[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5190ee3-6544-4ef3-9655-02ece1176d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.unsqueeze(image_tensor, -1)\n",
    "print(tensor.shape)\n",
    "print(tensor[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030faaa-f608-4ec8-952b-090ea177448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.Conv3d(1, 5, 5, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ab112f-8170-41f5-a81a-2ab2856d76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test(image_tensor.unsqueeze(0).unsqueeze(0))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afbca5-1ae3-4e90-bd2d-eb0e429f843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn_model():\n",
    "    in_channel = 1\n",
    "    base_out = 4\n",
    "    return nn.Sequential(nn.Conv3d(in_channels=in_channel, out_channels=base_out, kernel_size=3, bias=False),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out, base_out*2, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*2),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*2, base_out*4, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*4),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Conv3d(base_out*4, base_out*8, 3, bias=False),\n",
    "                         nn.MaxPool3d(2),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         \n",
    "                         \n",
    "                         nn.Conv3d(base_out*8, base_out*8, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*8),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*8, base_out*16, 3, bias=False),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.MaxPool3d(2),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         \n",
    "                         \n",
    "                         nn.Conv3d(base_out*16, base_out*16, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(base_out*16),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                         nn.Conv3d(base_out*16, base_out*32, 3, bias=False),\n",
    "                         nn.InstanceNorm3d(256),\n",
    "                         nn.LeakyReLU(),\n",
    "                         nn.Dropout3d(p=0.3),\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594587b-e6ef-43f5-bf45-01cd1a84fea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lin_model():\n",
    "    return nn.Sequential(nn.Linear(10616832, 64),\n",
    "#                          nn.Dropout2d(p=0.2),\n",
    "#                          nn.Linear(2048, 64),\n",
    "                         nn.Dropout2d(p=0.2),\n",
    "                         nn.Linear(64, 8),\n",
    "                         nn.Dropout2d(p=0.2),\n",
    "                         nn.Linear(8, 2)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e7aef-766a-48f0-9199-9436e9a88ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = make_nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7c1a3-799f-42c5-9eca-8a2bd1b1fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nn_model(image_tensor.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a47bc-48e3-4fe9-a0a6-c540e88b35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa582bb9-8fda-4eeb-9754-2f2edead0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.flatten(out,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8355d-2f92-4b16-aa0b-ca3f77338aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8988910-57c6-48c1-bbd0-b69f04bbde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = make_lin_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd4985-58fb-474f-9a93-4882df33ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = lin_model(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701c5d9-427d-47e9-a54d-0cb712c09028",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9f1cf4-c020-465f-99e7-15721eb61b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9bf33-03fa-4d47-ae32-c49931ca9afc",
   "metadata": {},
   "source": [
    "## Create Loss Fxn and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2362e391-34d7-4da2-9e92-8f9623f05e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db8ceb14-dfc3-4162-a237-f3e692eea66a",
   "metadata": {},
   "source": [
    "## Testing on First 100 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830e06c2-ea2c-4313-90ec-5a02449a0857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af3a404-d887-4828-b90f-ba3ae963f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(path+'000**/FLAIR.nii')\n",
    "files.extend(glob.glob(path+'001**/FLAIR.nii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04a6fa32-7b1d-4e82-a222-8f1ee57d9796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:/BT_Classification/backup/train\\\\00000\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00002\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00003\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00005\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00006\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00008\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00009\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00011\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00012\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00014\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00017\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00018\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00019\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00020\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00021\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00022\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00024\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00025\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00026\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00028\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00030\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00031\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00032\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00033\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00035\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00036\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00043\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00044\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00045\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00046\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00048\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00049\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00052\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00053\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00054\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00056\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00058\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00059\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00060\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00061\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00062\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00063\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00064\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00066\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00068\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00070\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00071\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00072\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00074\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00077\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00078\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00081\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00084\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00085\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00087\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00088\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00089\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00090\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00094\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00095\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00096\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00097\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00098\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00099\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00100\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00102\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00104\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00105\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00106\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00107\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00108\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00109\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00110\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00111\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00112\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00113\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00116\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00117\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00120\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00121\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00122\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00123\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00124\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00128\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00130\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00132\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00133\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00134\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00136\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00137\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00138\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00139\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00140\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00142\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00143\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00144\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00146\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00147\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00148\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00149\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00150\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00151\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00154\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00155\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00156\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00157\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00158\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00159\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00160\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00162\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00165\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00166\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00167\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00169\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00170\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00171\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00172\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00176\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00177\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00178\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00183\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00184\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00185\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00186\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00187\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00188\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00191\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00192\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00193\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00194\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00195\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00196\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00197\\\\FLAIR.nii', 'D:/BT_Classification/backup/train\\\\00199\\\\FLAIR.nii']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99f164e5-f6d2-4bdd-a251-5dfffb3c674b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 D:/BT_Classification/backup/train\\00000\\FLAIR.nii\n",
      "1 D:/BT_Classification/backup/train\\00002\\FLAIR.nii\n",
      "2 D:/BT_Classification/backup/train\\00003\\FLAIR.nii\n",
      "3 D:/BT_Classification/backup/train\\00005\\FLAIR.nii\n",
      "4 D:/BT_Classification/backup/train\\00006\\FLAIR.nii\n",
      "5 D:/BT_Classification/backup/train\\00008\\FLAIR.nii\n",
      "6 D:/BT_Classification/backup/train\\00009\\FLAIR.nii\n",
      "7 D:/BT_Classification/backup/train\\00011\\FLAIR.nii\n",
      "8 D:/BT_Classification/backup/train\\00012\\FLAIR.nii\n",
      "9 D:/BT_Classification/backup/train\\00014\\FLAIR.nii\n",
      "10 D:/BT_Classification/backup/train\\00017\\FLAIR.nii\n",
      "11 D:/BT_Classification/backup/train\\00018\\FLAIR.nii\n",
      "12 D:/BT_Classification/backup/train\\00019\\FLAIR.nii\n",
      "13 D:/BT_Classification/backup/train\\00020\\FLAIR.nii\n",
      "14 D:/BT_Classification/backup/train\\00021\\FLAIR.nii\n",
      "15 D:/BT_Classification/backup/train\\00022\\FLAIR.nii\n",
      "16 D:/BT_Classification/backup/train\\00024\\FLAIR.nii\n",
      "17 D:/BT_Classification/backup/train\\00025\\FLAIR.nii\n",
      "18 D:/BT_Classification/backup/train\\00026\\FLAIR.nii\n",
      "19 D:/BT_Classification/backup/train\\00028\\FLAIR.nii\n",
      "20 D:/BT_Classification/backup/train\\00030\\FLAIR.nii\n",
      "21 D:/BT_Classification/backup/train\\00031\\FLAIR.nii\n",
      "22 D:/BT_Classification/backup/train\\00032\\FLAIR.nii\n",
      "23 D:/BT_Classification/backup/train\\00033\\FLAIR.nii\n",
      "24 D:/BT_Classification/backup/train\\00035\\FLAIR.nii\n",
      "25 D:/BT_Classification/backup/train\\00036\\FLAIR.nii\n",
      "26 D:/BT_Classification/backup/train\\00043\\FLAIR.nii\n",
      "27 D:/BT_Classification/backup/train\\00044\\FLAIR.nii\n",
      "28 D:/BT_Classification/backup/train\\00045\\FLAIR.nii\n",
      "29 D:/BT_Classification/backup/train\\00046\\FLAIR.nii\n",
      "30 D:/BT_Classification/backup/train\\00048\\FLAIR.nii\n",
      "31 D:/BT_Classification/backup/train\\00049\\FLAIR.nii\n",
      "32 D:/BT_Classification/backup/train\\00052\\FLAIR.nii\n",
      "33 D:/BT_Classification/backup/train\\00053\\FLAIR.nii\n",
      "\tSkipped\n",
      "34 D:/BT_Classification/backup/train\\00054\\FLAIR.nii\n",
      "35 D:/BT_Classification/backup/train\\00056\\FLAIR.nii\n",
      "36 D:/BT_Classification/backup/train\\00058\\FLAIR.nii\n",
      "37 D:/BT_Classification/backup/train\\00059\\FLAIR.nii\n",
      "38 D:/BT_Classification/backup/train\\00060\\FLAIR.nii\n",
      "39 D:/BT_Classification/backup/train\\00061\\FLAIR.nii\n",
      "40 D:/BT_Classification/backup/train\\00062\\FLAIR.nii\n",
      "41 D:/BT_Classification/backup/train\\00063\\FLAIR.nii\n",
      "42 D:/BT_Classification/backup/train\\00064\\FLAIR.nii\n",
      "43 D:/BT_Classification/backup/train\\00066\\FLAIR.nii\n",
      "44 D:/BT_Classification/backup/train\\00068\\FLAIR.nii\n",
      "45 D:/BT_Classification/backup/train\\00070\\FLAIR.nii\n",
      "46 D:/BT_Classification/backup/train\\00071\\FLAIR.nii\n",
      "47 D:/BT_Classification/backup/train\\00072\\FLAIR.nii\n",
      "48 D:/BT_Classification/backup/train\\00074\\FLAIR.nii\n",
      "49 D:/BT_Classification/backup/train\\00077\\FLAIR.nii\n",
      "50 D:/BT_Classification/backup/train\\00078\\FLAIR.nii\n",
      "51 D:/BT_Classification/backup/train\\00081\\FLAIR.nii\n",
      "52 D:/BT_Classification/backup/train\\00084\\FLAIR.nii\n",
      "53 D:/BT_Classification/backup/train\\00085\\FLAIR.nii\n",
      "54 D:/BT_Classification/backup/train\\00087\\FLAIR.nii\n",
      "55 D:/BT_Classification/backup/train\\00088\\FLAIR.nii\n",
      "56 D:/BT_Classification/backup/train\\00089\\FLAIR.nii\n",
      "57 D:/BT_Classification/backup/train\\00090\\FLAIR.nii\n",
      "58 D:/BT_Classification/backup/train\\00094\\FLAIR.nii\n",
      "59 D:/BT_Classification/backup/train\\00095\\FLAIR.nii\n",
      "60 D:/BT_Classification/backup/train\\00096\\FLAIR.nii\n",
      "61 D:/BT_Classification/backup/train\\00097\\FLAIR.nii\n",
      "62 D:/BT_Classification/backup/train\\00098\\FLAIR.nii\n",
      "63 D:/BT_Classification/backup/train\\00099\\FLAIR.nii\n",
      "64 D:/BT_Classification/backup/train\\00100\\FLAIR.nii\n",
      "65 D:/BT_Classification/backup/train\\00102\\FLAIR.nii\n",
      "66 D:/BT_Classification/backup/train\\00104\\FLAIR.nii\n",
      "67 D:/BT_Classification/backup/train\\00105\\FLAIR.nii\n",
      "68 D:/BT_Classification/backup/train\\00106\\FLAIR.nii\n",
      "69 D:/BT_Classification/backup/train\\00107\\FLAIR.nii\n",
      "70 D:/BT_Classification/backup/train\\00108\\FLAIR.nii\n",
      "71 D:/BT_Classification/backup/train\\00109\\FLAIR.nii\n",
      "\tSkipped\n",
      "72 D:/BT_Classification/backup/train\\00110\\FLAIR.nii\n",
      "73 D:/BT_Classification/backup/train\\00111\\FLAIR.nii\n",
      "74 D:/BT_Classification/backup/train\\00112\\FLAIR.nii\n",
      "75 D:/BT_Classification/backup/train\\00113\\FLAIR.nii\n",
      "76 D:/BT_Classification/backup/train\\00116\\FLAIR.nii\n",
      "77 D:/BT_Classification/backup/train\\00117\\FLAIR.nii\n",
      "78 D:/BT_Classification/backup/train\\00120\\FLAIR.nii\n",
      "79 D:/BT_Classification/backup/train\\00121\\FLAIR.nii\n",
      "80 D:/BT_Classification/backup/train\\00122\\FLAIR.nii\n",
      "81 D:/BT_Classification/backup/train\\00123\\FLAIR.nii\n",
      "\tSkipped\n",
      "82 D:/BT_Classification/backup/train\\00124\\FLAIR.nii\n",
      "83 D:/BT_Classification/backup/train\\00128\\FLAIR.nii\n",
      "\tCrop size too large. Image:torch.Size([283, 259, 49]) \t crop:(50, 50, 50)\n",
      "84 D:/BT_Classification/backup/train\\00130\\FLAIR.nii\n",
      "85 D:/BT_Classification/backup/train\\00132\\FLAIR.nii\n",
      "86 D:/BT_Classification/backup/train\\00133\\FLAIR.nii\n",
      "87 D:/BT_Classification/backup/train\\00134\\FLAIR.nii\n",
      "88 D:/BT_Classification/backup/train\\00136\\FLAIR.nii\n",
      "89 D:/BT_Classification/backup/train\\00137\\FLAIR.nii\n",
      "90 D:/BT_Classification/backup/train\\00138\\FLAIR.nii\n",
      "91 D:/BT_Classification/backup/train\\00139\\FLAIR.nii\n",
      "92 D:/BT_Classification/backup/train\\00140\\FLAIR.nii\n",
      "93 D:/BT_Classification/backup/train\\00142\\FLAIR.nii\n",
      "94 D:/BT_Classification/backup/train\\00143\\FLAIR.nii\n",
      "95 D:/BT_Classification/backup/train\\00144\\FLAIR.nii\n",
      "96 D:/BT_Classification/backup/train\\00146\\FLAIR.nii\n",
      "97 D:/BT_Classification/backup/train\\00147\\FLAIR.nii\n",
      "98 D:/BT_Classification/backup/train\\00148\\FLAIR.nii\n",
      "99 D:/BT_Classification/backup/train\\00149\\FLAIR.nii\n",
      "100 D:/BT_Classification/backup/train\\00150\\FLAIR.nii\n",
      "101 D:/BT_Classification/backup/train\\00151\\FLAIR.nii\n",
      "102 D:/BT_Classification/backup/train\\00154\\FLAIR.nii\n",
      "103 D:/BT_Classification/backup/train\\00155\\FLAIR.nii\n",
      "104 D:/BT_Classification/backup/train\\00156\\FLAIR.nii\n",
      "105 D:/BT_Classification/backup/train\\00157\\FLAIR.nii\n",
      "106 D:/BT_Classification/backup/train\\00158\\FLAIR.nii\n",
      "107 D:/BT_Classification/backup/train\\00159\\FLAIR.nii\n",
      "108 D:/BT_Classification/backup/train\\00160\\FLAIR.nii\n",
      "109 D:/BT_Classification/backup/train\\00162\\FLAIR.nii\n",
      "110 D:/BT_Classification/backup/train\\00165\\FLAIR.nii\n",
      "111 D:/BT_Classification/backup/train\\00166\\FLAIR.nii\n",
      "112 D:/BT_Classification/backup/train\\00167\\FLAIR.nii\n",
      "113 D:/BT_Classification/backup/train\\00169\\FLAIR.nii\n",
      "114 D:/BT_Classification/backup/train\\00170\\FLAIR.nii\n",
      "115 D:/BT_Classification/backup/train\\00171\\FLAIR.nii\n",
      "116 D:/BT_Classification/backup/train\\00172\\FLAIR.nii\n",
      "117 D:/BT_Classification/backup/train\\00176\\FLAIR.nii\n",
      "118 D:/BT_Classification/backup/train\\00177\\FLAIR.nii\n",
      "119 D:/BT_Classification/backup/train\\00178\\FLAIR.nii\n",
      "120 D:/BT_Classification/backup/train\\00183\\FLAIR.nii\n",
      "121 D:/BT_Classification/backup/train\\00184\\FLAIR.nii\n",
      "122 D:/BT_Classification/backup/train\\00185\\FLAIR.nii\n",
      "123 D:/BT_Classification/backup/train\\00186\\FLAIR.nii\n",
      "\tSkipped\n",
      "124 D:/BT_Classification/backup/train\\00187\\FLAIR.nii\n",
      "125 D:/BT_Classification/backup/train\\00188\\FLAIR.nii\n",
      "126 D:/BT_Classification/backup/train\\00191\\FLAIR.nii\n",
      "127 D:/BT_Classification/backup/train\\00192\\FLAIR.nii\n",
      "128 D:/BT_Classification/backup/train\\00193\\FLAIR.nii\n",
      "129 D:/BT_Classification/backup/train\\00194\\FLAIR.nii\n",
      "130 D:/BT_Classification/backup/train\\00195\\FLAIR.nii\n",
      "131 D:/BT_Classification/backup/train\\00196\\FLAIR.nii\n",
      "132 D:/BT_Classification/backup/train\\00197\\FLAIR.nii\n",
      "133 D:/BT_Classification/backup/train\\00199\\FLAIR.nii\n"
     ]
    }
   ],
   "source": [
    "crops, train_data = get_random_crops(files, crop_size=(50, 50, 50), count=1, label_path=path+'/../../train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d21cc9-3805-4122-a51b-be90fd6da0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "print(len(crops))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ecd35da-7d20-4f31-bcca-f367a05ad7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.in_channel = 1\n",
    "        self.base_channel = 4\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=self.in_channel, out_channels=self.base_channel, kernel_size=3, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout3d(p=0.3),\n",
    "            nn.Conv3d(self.base_channel, self.base_channel*2, 3, bias=False),\n",
    "            nn.InstanceNorm3d(self.base_channel*2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout3d(p=0.3),\n",
    "            nn.Conv3d(self.base_channel*2, self.base_channel*4, 3, bias=False),\n",
    "            nn.InstanceNorm3d(self.base_channel*4),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout3d(p=0.3),\n",
    "            nn.Conv3d(self.base_channel*4, self.base_channel*8, 3, bias=False),\n",
    "            nn.MaxPool3d(2),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv3d(self.base_channel*8, self.base_channel*8, 3, bias=False),\n",
    "            nn.InstanceNorm3d(self.base_channel*8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout3d(p=0.3),\n",
    "            nn.Conv3d(self.base_channel*8, self.base_channel*16, 3, bias=False),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool3d(2),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv3d(self.base_channel*16, self.base_channel*16, 3, bias=False),\n",
    "            nn.InstanceNorm3d(self.base_channel*16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout3d(p=0.3),\n",
    "            nn.Conv3d(self.base_channel*16, self.base_channel*32, 3, bias=False),\n",
    "            nn.InstanceNorm3d(self.base_channel*32),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(8192, 64),\n",
    "#             nn.Dropout2d(p=0.2),\n",
    "#             nn.Linear(2048, 64),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.Linear(64, 8),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out2 = self.conv2(out1)\n",
    "        out3 = self.conv3(out2)\n",
    "        \n",
    "        out3 = torch.flatten(out3,1)\n",
    "        \n",
    "        out4 = self.lin(out3)\n",
    "        \n",
    "        return out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "419df9f4-a417-4ba8-95e3-cfc144582c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a310cdd-47ea-4080-92dc-0822f4a64510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbc59e31-2905-4104-8f02-8aa746b30e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6c53e2d-b1b4-4dee-bf18-4d9bd8b29507",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::slow_conv3d_forward' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::slow_conv3d_forward' is only available for these backends: [CPU, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:16286 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradMLC: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_4.cpp:9909 [kernel]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:255 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5f644c81c997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# forward + backward + optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-2613ff61d126>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mout1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mout2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mout3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    581\u001b[0m             )\n\u001b[0;32m    582\u001b[0m         return F.conv3d(\n\u001b[1;32m--> 583\u001b[1;33m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m         )\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'aten::slow_conv3d_forward' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::slow_conv3d_forward' is only available for these backends: [CPU, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at aten\\src\\ATen\\RegisterCPU.cpp:16286 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:60 [backend fallback]\nAutogradOther: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradCPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradCUDA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradXLA: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nUNKNOWN_TENSOR_TYPE_ID: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradMLC: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradHPU: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradNestedTensor: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse1: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse2: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nAutogradPrivateUse3: registered at ..\\torch\\csrc\\autograd\\generated\\VariableType_4.cpp:9226 [autograd kernel]\nTracer: registered at ..\\torch\\csrc\\autograd\\generated\\TraceType_4.cpp:9909 [kernel]\nAutocast: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:255 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\BatchingRegistrations.cpp:1019 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\" \n",
    "else:\n",
    "    dev = 'cpu'\n",
    "\n",
    "net.to('cpu')\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(train_data, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs.unsqueeze(1))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    print('[%d] loss: %.3f' %\n",
    "          (epoch + 1, loss.item()))\n",
    "        \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        }, path+'/../../checkpoints/cp_{}.cp'.format(epoch))\n",
    "\n",
    "print('Finished Training')\n",
    "torch.save(net.state_dict(), path+'/../model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3c65a-ed5b-4ba0-b8d0-073031b3e9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
